I'm starting a backprop project. Expect to see a lot of Gradients, Hessians, JAX and PyTorch.

See you!

I'll be working daily on this project.
 
### Abstract:
My main goal is to learn Pytorch/Jax, but also have a clear vision on what's happening underneath. I intend to build and train NNs from scratch, but have a PyTorch/Jax implelemention in parallel for correctness. Similar to what Andrej Karpathy have done in [Micrograd](https://github.com/karpathy/micrograd) with Pytorch.


### Goals:

[1] Be able to build and train NNs from scratch.
[2] Focus on Modularity, and Optimization of code.
[3] Learn & check correctness with PyTorch/Jax.
[4] See how it is done PyTorch/Jax on a deeper level.


### Steps:
[1] Build a starting NN:
    [1] Set up a working env:
        * Load MNIST
    [2] Train a simple MLP to classify MNIST
        * Forward.
        * Backward.
        * Loop and make sure the NN is learning.
    [DONE] Current accuracy with a simple MLP is 0.934.
    [3] Learn how to load the data faster.
    [4] Try a softmax layer
    [5] Vertorize the tensor manipulations (the softmax layer)
[2] Make it modular.
[3] Start the project.


### References:
[1] Andrej Karpathy
[2] Grokking DL
[3] PyTorch / Jax

